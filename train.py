import os, json, time, copy
from collections import defaultdict

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset, random_split
import numpy as np
import matplotlib.pyplot as plt

# ğŸ”¹ ì „ëµ í•¨ìˆ˜ë“¤ (ì „ëµ1/2/3) ë¶ˆëŸ¬ì˜¤ê¸°
from pruning.strategies import (
    compute_dynamic_ratios_vanilla,
    compute_dynamic_ratios_p,
    compute_dynamic_ratios_beta,
)

# -------------------------------
# 0) ì„¤ì •
# -------------------------------
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… Device:", device)

MODEL_ID = "resnet18"

from google.colab import drive; drive.mount('/content/drive')
CHECKPOINT_DIR = "/content/drive/MyDrive/ckpt_block_sweep"
RESULTS_JSON = os.path.join(CHECKPOINT_DIR, f"{MODEL_ID}_per_block_results.json")
BASE_MODEL_PATH = os.path.join(CHECKPOINT_DIR, f"{MODEL_ID}_base.pth")

USE_AMP = torch.cuda.is_available()
BATCH_SIZE = 128

# -------------------------------
# 1) ë°ì´í„°: CIFAR-100 (Validation Set ë¶„ë¦¬)
# -------------------------------
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),
])
transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),
])

full_trainset = torchvision.datasets.CIFAR100(
    root="./data", train=True, download=True, transform=transform_train
)
train_size = int(0.9 * len(full_trainset))
val_size = len(full_trainset) - train_size
trainset, validationset = random_split(full_trainset, [train_size, val_size])

testset = torchvision.datasets.CIFAR100(
    root="./data", train=False, download=True, transform=transform_test
)

trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)
validationloader = DataLoader(validationset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
testloader = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)


# -------------------------------
# 2) ìœ í‹¸
# -------------------------------
def count_parameters(model):
    """ëª¨ë¸ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


# -------------------------------
# 3) í•™ìŠµ/í‰ê°€ ë£¨í”„
# -------------------------------
if USE_AMP:
    scaler = torch.amp.GradScaler(device.type)
else:
    scaler = None


def train_one_epoch(model, loader, optimizer, criterion, epoch, scheduler=None):
    model.train()
    total_loss = 0.0
    for x, y in loader:
        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
        optimizer.zero_grad(set_to_none=True)
        if USE_AMP:
            with torch.amp.autocast(device_type=device.type):
                out = model(x)
                loss = criterion(out, y)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            out = model(x)
            loss = criterion(out, y)
            loss.backward()
            optimizer.step()
        total_loss += loss.item()
    if scheduler:
        scheduler.step()
    print(f"ğŸ“˜ Epoch {epoch} | Loss: {total_loss/len(loader):.4f}")


@torch.no_grad()
def test(model, loader):
    model.eval()
    correct, total = 0, 0
    for x, y in loader:
        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
        out = model(x)
        pred = out.argmax(1)
        correct += (pred == y).sum().item()
        total += y.size(0)
    acc = 100.0 * correct / total
    return acc


# -------------------------------
# 4) ëª¨ë¸ íŒ©í† ë¦¬ & ë¸”ë¡ íƒìƒ‰
# -------------------------------
def build_base_resnet18(num_classes=100):
    m = torchvision.models.resnet18(weights=None)
    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    m.maxpool = nn.Identity()
    m.fc = nn.Linear(m.fc.in_features, num_classes)
    return m.to(device)


def find_block_modules(model):
    return {
        n: md
        for n, md in model.named_modules()
        if isinstance(md, torchvision.models.resnet.BasicBlock)
    }


# -------------------------------
# 5) í”„ë£¨ë‹ ìœ í‹¸
# -------------------------------
def prune_conv_and_bn(conv, bn, keep_idx, in_keep_idx=None):
    if not keep_idx:
        raise ValueError("Pruning produced zero output channels.")

    W = conv.weight.data.clone()
    B = conv.bias.data.clone() if conv.bias is not None else None

    W = W[keep_idx, :, :, :]
    if B is not None:
        B = B[keep_idx]
    if in_keep_idx is not None:
        W = W[:, in_keep_idx, :, :]

    new_conv = nn.Conv2d(
        in_channels=W.shape[1],
        out_channels=W.shape[0],
        kernel_size=conv.kernel_size,
        stride=conv.stride,
        padding=conv.padding,
        dilation=conv.dilation,
        groups=conv.groups,
        bias=(conv.bias is not None),
    )
    new_conv.weight.data = W.clone()
    if B is not None:
        new_conv.bias.data = B.clone()

    new_bn = nn.BatchNorm2d(len(keep_idx))
    new_bn.weight.data = bn.weight.data[keep_idx].clone()
    new_bn.bias.data = bn.bias.data[keep_idx].clone()
    new_bn.running_mean = bn.running_mean[keep_idx].clone()
    new_bn.running_var = bn.running_var[keep_idx].clone()

    return new_conv, new_bn


def prune_resnet_blockwise(model, block_keep_indices):
    model = copy.deepcopy(model).cpu()
    block_items = [
        (n, m)
        for n, m in model.named_modules()
        if isinstance(m, torchvision.models.resnet.BasicBlock)
    ]

    prev_out_indices = list(range(model.conv1.out_channels))

    for name, block in block_items:
        in_keep_idx = prev_out_indices
        keep_idx = block_keep_indices.get(
            name, list(range(block.conv1.out_channels))
        )

        new_conv1, new_bn1 = prune_conv_and_bn(block.conv1, block.bn1, keep_idx, in_keep_idx)
        block.conv1 = new_conv1
        block.bn1 = new_bn1

        new_conv2, new_bn2 = prune_conv_and_bn(block.conv2, block.bn2, keep_idx, keep_idx)
        block.conv2 = new_conv2
        block.bn2 = new_bn2

        if block.downsample is not None:
            ds_conv, ds_bn = block.downsample[0], block.downsample[1]
            new_ds_conv, new_ds_bn = prune_conv_and_bn(ds_conv, ds_bn, keep_idx, in_keep_idx)
            block.downsample = nn.Sequential(new_ds_conv, new_ds_bn)
        elif len(in_keep_idx) != len(keep_idx):
            block.downsample = nn.Sequential(
                nn.Conv2d(
                    len(in_keep_idx),
                    len(keep_idx),
                    kernel_size=1,
                    stride=block.conv1.stride,
                    bias=False,
                ),
                nn.BatchNorm2d(len(keep_idx)),
            )
        prev_out_indices = keep_idx

    last_block = block_items[-1][1]
    final_in_features = last_block.bn2.num_features
    model.fc = nn.Linear(final_in_features, 100)

    return model.to(device)


# -------------------------------
# 6) ë©”ì¸ í•¨ìˆ˜ (ì „ëµ ì„ íƒ ê°€ëŠ¥ ë²„ì „)
# -------------------------------
def main_adaptive_iterative_pruning(
    n_rounds=10,
    finetune_epochs_per_round=5,
    final_finetune_epochs=10,
    alpha=0.1,
    strategy="vanilla",          # "vanilla", "p", "beta"
    p=2.5,                       # ì „ëµ 2ì—ì„œ ì‚¬ìš©í•˜ëŠ” p
    beta=0.5,                    # ì „ëµ 3ì—ì„œ ì‚¬ìš©í•˜ëŠ” beta
    GLOBAL_PRUNING_TARGET_RATIO=60.0,  # ê¸°ë³¸ì€ ì „ëµ1ì—ì„œ ë„¤ê°€ ì“´ 60%
):
    """
    ì „ëµ 1/2/3ì„ ì„ íƒí•´ì„œ ë°˜ë³µ í”„ë£¨ë‹ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"===== âœ¨ ì „ëµ: {strategy} | ì´ {n_rounds} ë¼ìš´ë“œ, alpha={alpha} =====")

    base_model = build_base_resnet18(100)
    base_model.load_state_dict(torch.load(BASE_MODEL_PATH, map_location=device))
    base_params = count_parameters(base_model)
    print(f"âœ… ê¸°ì¤€ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {base_params:,}")
    print("--- ê¸°ì¤€ ëª¨ë¸ Test Accuracy ---")
    last_accuracy = test(base_model, testloader)
    print(f"ğŸ¯ Test Accuracy: {last_accuracy:.2f}%")

    pruned_model = copy.deepcopy(base_model)

    # ğŸ”¹ ë¯¼ê°ë„ / í•„í„° ì •ë³´ (ë„¤ê°€ ì“°ë˜ ê±° ê·¸ëŒ€ë¡œ)
    sensitivity_si = {
        'layer1.0': 0.0221, 'layer1.1': 0.0551,
        'layer2.0': 0.1250, 'layer2.1': 0.0319,
        'layer3.0': 0.1564, 'layer3.1': 0.1011,
        'layer4.0': 0.0768, 'layer4.1': 0.0488
    }
    filter_counts_Ni_original = {
        'layer1.0': 64, 'layer1.1': 64,
        'layer2.0': 128, 'layer2.1': 128,
        'layer3.0': 256, 'layer3.1': 256,
        'layer4.0': 512, 'layer4.1': 512
    }

    # ğŸ”¹ p / beta ì „ëµì—ì„œ ì‚¬ìš©í•˜ëŠ” "ë¸”ë¡ë³„ íŒŒë¼ë¯¸í„° ìˆ˜"
    all_blocks = find_block_modules(base_model)
    param_counts_Ni = {
        name: count_parameters(block) for name, block in all_blocks.items()
    }
    total_block_params = sum(param_counts_Ni.values())

    epsilon = 1e-8

    for i in range(1, n_rounds + 1):
        print(f"\nğŸ”¥ ë¼ìš´ë“œ {i}/{n_rounds}")

        # --- í•µì‹¬ ìŠ¤ì½”ì–´ í•¨ìˆ˜ ë¡œì§: ì—¬ê¸°ì„œ ì „ëµë§Œ ë‹¤ë¥´ê²Œ í˜¸ì¶œ ---
        if strategy == "vanilla":
            dynamic_ratios = compute_dynamic_ratios_vanilla(
                round_idx=i,
                n_rounds=n_rounds,
                sensitivity_si=sensitivity_si,
                filter_counts_Ni_original=filter_counts_Ni_original,
                global_target_ratio=GLOBAL_PRUNING_TARGET_RATIO,
            )
        elif strategy == "p":
            dynamic_ratios = compute_dynamic_ratios_p(
                round_idx=i,
                n_rounds=n_rounds,
                sensitivity_si=sensitivity_si,
                param_counts_Ni=param_counts_Ni,
                total_block_params=total_block_params,
                global_target_ratio=GLOBAL_PRUNING_TARGET_RATIO,
                p=p,
            )
        elif strategy == "beta":
            dynamic_ratios = compute_dynamic_ratios_beta(
                round_idx=i,
                n_rounds=n_rounds,
                sensitivity_si=sensitivity_si,
                param_counts_Ni=param_counts_Ni,
                total_block_params=total_block_params,
                global_target_ratio=GLOBAL_PRUNING_TARGET_RATIO,
                beta=beta,
            )
        else:
            raise ValueError(f"Unknown strategy: {strategy}")
        # --- í•µì‹¬ ë¡œì§ ë ---

        # ğŸ”¹ ê³µí†µ: dynamic_ratios â†’ ë‚¨ê¸¸ í•„í„° ì¸ë±ìŠ¤ ê³„ì‚°
        global_keep_indices = {}
        print("\n[ì´ë²ˆ ë¼ìš´ë“œì— ì ìš©í•  í”„ë£¨ë‹ ì¸ë±ìŠ¤ ê³„ì‚°]")
        for block_name in sorted(filter_counts_Ni_original.keys()):
            ratio_to_prune = dynamic_ratios.get(block_name, 0.0)
            original_C = filter_counts_Ni_original[block_name]
            num_to_keep = int(original_C * (1.0 - ratio_to_prune / 100.0))
            num_to_keep = max(1, num_to_keep)

            all_modules_current = find_block_modules(pruned_model)
            block = all_modules_current[block_name]
            imp = block.bn1.weight.data.abs().cpu()

            order = imp.argsort(descending=True)
            keep_idx_target = sorted(order[:num_to_keep].tolist())
            global_keep_indices[block_name] = keep_idx_target
            print(f"ë¸”ë¡: {block_name:<15} | ì›ë³¸ í•„í„°: {original_C} -> ë‚¨ê¸¸ í•„í„°: {len(keep_idx_target)}")

        # ğŸ”¹ ì‹¤ì œ í”„ë£¨ë‹ ì ìš©
        pruned_model = prune_resnet_blockwise(base_model, global_keep_indices)

        # ğŸ”¹ ë¼ìš´ë“œë§ˆë‹¤ íŒŒì¸íŠœë‹
        optimizer = optim.SGD(pruned_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
        criterion = nn.CrossEntropyLoss()

        print(f"\nğŸš€ ë¼ìš´ë“œ {i} Fine-tune ì‹œì‘ (ì´ {finetune_epochs_per_round} ì—í¬í¬)")
        for ep in range(1, finetune_epochs_per_round + 1):
            train_one_epoch(pruned_model, trainloader, optimizer, criterion, ep)

        print(f"\n--- ë¼ìš´ë“œ {i} ì¤‘ê°„ í‰ê°€ (Validation Set) ---")
        current_accuracy = test(pruned_model, validationloader)
        print(f"ğŸ¯ Validation Accuracy: {current_accuracy:.2f}%")
        accuracy_drop = max(0, last_accuracy - current_accuracy)
        print(f"ğŸ§  ì´ì „ ì •í™•ë„: {last_accuracy:.2f}% -> í˜„ì¬ ì •í™•ë„: {current_accuracy:.2f}% (í•˜ë½í­: {accuracy_drop:.2f}%)")

        update_value = accuracy_drop * alpha
        if update_value > 0:
            print(f"ğŸ§  ë¯¼ê°ë„ ìŠ¤ì½”ì–´ë¥¼ {update_value:.4f} ë§Œí¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
            for block_name in sensitivity_si:
                sensitivity_si[block_name] += update_value

        last_accuracy = current_accuracy

    print("\nâœ… ëª¨ë“  í”„ë£¨ë‹ ì™„ë£Œ. ìµœì¢… ì¬í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    FINAL_RETRAIN_EPOCHS = final_finetune_epochs

    optimizer = optim.SGD(pruned_model.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4)
    criterion = nn.CrossEntropyLoss()
    for ep in range(1, FINAL_RETRAIN_EPOCHS + 1):
        train_one_epoch(pruned_model, trainloader, optimizer, criterion, ep)

    print("\n--- ìµœì¢… ê²½ëŸ‰í™” ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ---")
    final_pruned_params = count_parameters(pruned_model)
    final_compression_rate = (1 - final_pruned_params / base_params) * 100
    print(f"ğŸ—œï¸ ìµœì¢… ì••ì¶•ë¥ : {final_compression_rate:.2f}%")

    print("\n--- ìµœì¢… Validation Accuracy ---")
    final_val_acc = test(pruned_model, validationloader)
    print(f"ğŸ¯ Validation Accuracy: {final_val_acc:.2f}%")

    print("\n--- ìµœì¢… Test Accuracy ---")
    final_test_acc = test(pruned_model, testloader)
    print(f"ğŸ¯ Test Accuracy: {final_test_acc:.2f}%")


# -------------------------------
# 7) ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# -------------------------------
if __name__ == "__main__":
    # ğŸ”¹ ì „ëµ 1: p / beta ì—†ëŠ” ê¸°ë³¸ ë²„ì „ (ë„¤ê°€ ë§ˆì§€ë§‰ì— ì¤€ ì½”ë“œë‘ ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ë§ì¶¤)
    main_adaptive_iterative_pruning(
        n_rounds=10,
        finetune_epochs_per_round=5,
        final_finetune_epochs=10,
        alpha=0.1,
        strategy="vanilla",
        GLOBAL_PRUNING_TARGET_RATIO=60.0,
    )

    # ğŸ”¹ ì „ëµ 2 ì˜ˆì‹œ: p ì „ëµ ì‹¤í—˜í•˜ê³  ì‹¶ì„ ë•Œ
    # main_adaptive_iterative_pruning(
    #     strategy="p",
    #     p=2.5,
    #     GLOBAL_PRUNING_TARGET_RATIO=80.0,
    # )

    # ğŸ”¹ ì „ëµ 3 ì˜ˆì‹œ: beta ì „ëµ ì‹¤í—˜í•˜ê³  ì‹¶ì„ ë•Œ
    # main_adaptive_iterative_pruning(
    #     strategy="beta",
    #     beta=0.8,
    #     GLOBAL_PRUNING_TARGET_RATIO=80.0,
    # )
